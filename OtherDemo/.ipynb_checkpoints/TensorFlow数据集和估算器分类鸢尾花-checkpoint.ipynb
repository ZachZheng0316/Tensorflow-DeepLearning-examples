{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载库文件并判断版本\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if sys.version_info < (3, 0, 0):\n",
    "    from urllib import urlopen\n",
    "else:\n",
    "    from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "# Check that we have correct TensorFlow version installed\n",
    "tf_version = tf.__version__\n",
    "print(\"TensorFlow version: {}\".format(tf_version))\n",
    "assert \"1.3\" <= tf_version, \"TensorFlow r1.3 or later is needed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Windows users：You only need to change PATH, rest is platform independent\n",
    "PATH = \"../../../TensorFlow/checkpoint/tf_dataset_and_estimator_spi\"\n",
    "\n",
    "# Fetch and store Training and Test dataset files\n",
    "PATH_DATASET = \"../../../TensorFlow/datasets/\"\n",
    "FILE_TRAIN = PATH_DATASET + os.sep + \"iris_training.csv\"\n",
    "FILE_TEST = PATH_DATASET + os.sep + \"iris_test.csv\"\n",
    "URL_TRAIN = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "URL_TEST = \"http://download.tensorflow.org/data/iris_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 下载数据集\n",
    "def downloadDataset(url, path):\n",
    "    if not os.path.exists(PATH_DATASET):\n",
    "        os.makedirs(PATH_DATASET)\n",
    "    if not os.path.exists(path):\n",
    "        data = urlopen(url).read()\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(data)\n",
    "            f.close()\n",
    "\n",
    "downloadDataset(URL_TRAIN, FILE_TRAIN)\n",
    "downloadDataset(URL_TEST, FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设置记录消息的阈值\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the CSV feature in our training & test data\n",
    "feature_names = ['SepalLength','SepalWidth','PetalLength','PetalWidth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def my_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
    "    # 定义解码函数\n",
    "    def decode_csv(line):\n",
    "        # 把line(文本中的每一行数据)分解成四个float, 一个int类型数据\n",
    "        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]]) # 最后一个元素为int类型\n",
    "        \n",
    "        # 获取label\n",
    "        label = parsed_line[-1] # Last element is the label\n",
    "        del parsed_line[-1] # Delete last element\n",
    "        \n",
    "        # 获取样本特征\n",
    "        features = parsed_line # Everything but last elements are the features\n",
    "        \n",
    "        # 返回特征数据和label\n",
    "        d = dict(zip(feature_names, features)), label\n",
    "        return d\n",
    "    \n",
    "    # 读入文件路径并解析数据\n",
    "    dataset = (tf.data.TextLineDataset(file_path) # Read text file\n",
    "              .skip(1) # Skip header row\n",
    "              .map(decode_csv)) # Transform each elem by applying decode_csv fn\n",
    "    \n",
    "    # 是否打乱数据\n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "        \n",
    "    # 设置重复次数\n",
    "    dataset = dataset.repeat(repeat_count) # Repeats dataset this # times\n",
    "    \n",
    "    # 设置batch大小\n",
    "    dataset = dataset.batch(32) # Batch size to use\n",
    "    \n",
    "    # 生成数据迭代器\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    # 获取一个batch的数据\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    \n",
    "    # 返回一个batch的数据\n",
    "    return batch_features, batch_labels\n",
    "\n",
    "# Will return 32 random elements\n",
    "next_batch = my_input_fn(FILE_TRAIN, perform_shuffle=True, repeat_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the feature_columns, which specifies the input to our model\n",
    "# All our input features are numeric, so use numeric_column for each one\n",
    "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_is_chief': True, '_num_ps_replicas': 0, '_session_config': None, '_task_id': 0, '_save_checkpoints_steps': None, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001ED563BCA20>, '_master': '', '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '../../../TensorFlow/checkpoint/tf_dataset_and_estimator_spi', '_save_summary_steps': 100, '_tf_random_seed': None, '_num_worker_replicas': 1, '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_service': None, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Create a deep neural network regression classifier\n",
    "# Use the DNNClassifier pre-made estimator\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns, # The input features to our model\n",
    "    hidden_units=[10, 10], # Two layers, each with 10 neurons\n",
    "    n_classes=3,\n",
    "    model_dir=PATH) # Path to where checkpoint ets are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ../../../TensorFlow/checkpoint/tf_dataset_and_estimator_spi\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 107.5208\n",
      "INFO:tensorflow:Saving checkpoints for 30 into ../../../TensorFlow/checkpoint/tf_dataset_and_estimator_spi\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 18.9656.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x1ed6297b630>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train our model, use the previously function my_input_fn\n",
    "# Input to training is a file with training example\n",
    "# Stop training after 8 iterations of train data (epochs)\n",
    "classifier.train(input_fn=lambda: my_input_fn(FILE_TRAIN, True, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate our model using the examples contained in FILE_TEST\n",
    "# Return value will contain evaluation_metrics sunch as: loss & average_loss\n",
    "evaluate_result = classifier.evaluate(input_fn=lambda: my_input_fn(FILE_TEST, False, 1))\n",
    "print(\"Evaluation results: \")\n",
    "for key in evaluate_result:\n",
    "    print(\"  {}, was: {}\". format(key, evaluate_result[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the type of some Iris flowers.\n",
    "# Let's predict the examples in FILE_TEST, repeat ony once\n",
    "predict_results = classifier.predict(\n",
    "    input_fn=lambda: my_input_fn(FILE_TEST, False, 1))\n",
    "print(\"Predictions on test file: \")\n",
    "for prediction in predict_results:\n",
    "    # Will print the predicted class, i.e: 0, 1, or 2 if the prediction\n",
    "    # is Iris Sentosa, Vericolor, Virginica, respectively\n",
    "    print(prediction[\"class_ids\"][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let create a dataset for prediction\n",
    "# We've taken the first 3 examples in FILE_TEST\n",
    "prediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
    "                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris :\n",
    "                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Sentosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_input_fn():\n",
    "    def decode(x):\n",
    "        x = tf.split(x, 4)  # Need to split into our 4 features\n",
    "        return dict(zip(feature_names, x))  # To build a dict of them\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(prediction_input)\n",
    "    dataset = dataset.map(decode)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_feature_batch = iterator.get_next()\n",
    "    return next_feature_batch, None  # In prediction, we have no labels\n",
    "\n",
    "# Predict all our prediction_input\n",
    "predict_results = classifier.predict(input_fn=new_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"Predictions on memory\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "    type = prediction[\"class_ids\"][0]  # Get the predicted class (index)\n",
    "    if type == 0:\n",
    "        print(\"I think: {}, is Iris Sentosa\".format(prediction_input[idx]))\n",
    "    elif type == 1:\n",
    "        print(\"I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n",
    "    else:\n",
    "        print(\"I think: {}, is Iris Virginica\".format(prediction_input[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=../../../practise/tf_dataset_and_estimator_spi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
