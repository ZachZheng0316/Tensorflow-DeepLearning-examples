{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#词向量的概念\" data-toc-modified-id=\"词向量的概念-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>词向量的概念</a></span><ul class=\"toc-item\"><li><span><a href=\"#one-hot编码存在的问题\" data-toc-modified-id=\"one-hot编码存在的问题-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>one-hot编码存在的问题</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Word2Vec</a></span></li></ul></li><li><span><a href=\"#词向量的实现方法\" data-toc-modified-id=\"词向量的实现方法-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>词向量的实现方法</a></span><ul class=\"toc-item\"><li><span><a href=\"#CBOW\" data-toc-modified-id=\"CBOW-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>CBOW</a></span></li><li><span><a href=\"#Skip-Gram\" data-toc-modified-id=\"Skip-Gram-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Skip-Gram</a></span></li></ul></li><li><span><a href=\"#TensorFlow实现词向量\" data-toc-modified-id=\"TensorFlow实现词向量-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TensorFlow实现词向量</a></span><ul class=\"toc-item\"><li><span><a href=\"#载入库文件\" data-toc-modified-id=\"载入库文件-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>载入库文件</a></span></li><li><span><a href=\"#收集数据\" data-toc-modified-id=\"收集数据-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>收集数据</a></span><ul class=\"toc-item\"><li><span><a href=\"#下载数据\" data-toc-modified-id=\"下载数据-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>下载数据</a></span></li><li><span><a href=\"#解压数据\" data-toc-modified-id=\"解压数据-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>解压数据</a></span></li><li><span><a href=\"#建立词汇表\" data-toc-modified-id=\"建立词汇表-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>建立词汇表</a></span></li></ul></li><li><span><a href=\"#生成训练样本\" data-toc-modified-id=\"生成训练样本-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>生成训练样本</a></span><ul class=\"toc-item\"><li><span><a href=\"#算法流程\" data-toc-modified-id=\"算法流程-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>算法流程</a></span></li><li><span><a href=\"#测试generate_batch()函数产生的样例\" data-toc-modified-id=\"测试generate_batch()函数产生的样例-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>测试generate_batch()函数产生的样例</a></span></li></ul></li></ul></li><li><span><a href=\"#参考资料\" data-toc-modified-id=\"参考资料-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>参考资料</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量的概念\n",
    "\n",
    "## one-hot编码存在的问题\n",
    "\n",
    "自然语言处理在Word2Vec出现之前，通常将字词转成离散的单独的符号，比如将“中国”转为编号为5178的特征，将“北京”转为编号为3987的特征。这即是One-Hot Encoder，一个词对应一个向量（向量中只有一个值为1，其余为0），通常需要将一篇文章中每一个词都转成一个向量，而整篇文章则变为一个稀疏矩阵。\n",
    "\n",
    "对文本分类模型，我们使用Bag of Words模型，将文章对应的稀疏矩阵合并为一个向量，即把每一个词对应的向量加到一起，这样只统计每个词出现的次数，比如“中国”出现23次，那么第5178个特征为23，“北京”出现2次，那么第3987个特征为2。\n",
    "\n",
    "使用One-Hot Encoder存在以下两个问题\n",
    "\n",
    "1. 特征编码是随机的, 没有考虑字词间存在的关系.例如，我们对“中国”和“北京”的从属关系、地理位置关系等一无所知，我们从5178和3987这两个值看不出任何信息。\n",
    "2. 训练效率低, 计算麻烦.将字词存储为稀疏向量的话，我们通常需要更多的数据来训练，因为稀疏数据训练的效率比较低，计算也非常麻烦.\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "Word2Vec则是将语言中的字词转化为计算机可以理解的稠密向量（DenseVector）.\n",
    "\n",
    "Word2Vec 会将每一个单词表示成一个相对较低维度的向量(比如100维或者200维)。对于语义相近的词，其对应的单词向量在空间中的距离也应该接近。于是单词语义上的相似度可以通过空间距离来描述。单词向量不需要通过人工的方式设定，它可以从互联网上海量非标注文本中学习得到。使用斯坦福大学开源的 GloVe 单词向量可以得到与单词 \"frog\" (青蛙)所对应的单词向量最相似的5个单词分别是 \"frogs (青蛙复数)\"，\"toad(蟾蜍)\"，“litoria(雨滨蛙属)”，“leptodactylidae (细趾蟾科)”和 “rana (中国林蛙)”。从这个样例可以看出，单词向量可以非常有效的刻画单词的语义。通过单词向量还可以进行单词之间的运算。比如用单词“king”所代表的向量去减去\"man\"所代表的向量得到的结果和单词 \"queen\" 减去 \"woman\" 得到的结果向量是相似的。这说明在单词向量中，已经隐含了表达性别的概念.\n",
    "\n",
    "使用Word2Vec学习到的字词间对应关系如下:\n",
    "\n",
    "![6_1](6_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量的实现方法\n",
    "\n",
    "Word2Vec 即是一种计算非常高效的，可以从原始语料中学习字词空间向量的预测模型。它主要分为CBOW（ContinuousBag of Words）和 Skip-Gram 两种模式，其中CBOW是从原始语句（比如：中国的首都是____）推测目标字词（比如：北京）；而 Skip-Gram 则正好相反，它是从目标字词推测出原始语句. 其中CBOW对小型数据比较合适，而Skip-Gram在大型语料中表现得更好.\n",
    "\n",
    "## CBOW\n",
    "\n",
    "预测模型 Neural Probabilistic Language Models 通常使用最大似然的方法，在给定前面的语句 h 的情况下，最大化目标词汇 wt 的概率。但它存在的一个比较严重的问题是计算量非常大，需要计算词汇表中所有单词出现的可能性。在 Word2Vec 的CBOW模型中，不需要计算完整的概率模型，只需要训练一个二元的分类模型，用来区分真实的目标词汇和编造的词汇（噪声）这两类。这种用少量噪声词汇来估计的方法，类似于蒙特卡洛模拟。CBOW 模型如下所示:\n",
    "\n",
    "![6_2](6_2.png)\n",
    "\n",
    "当模型预测真实的目标词汇为高概率，同时预测其他噪声词汇为低概率时，我们训练的学习目标就被最优化了。用编造的噪声词汇训练的方法被称为 Negative Sampling。用这种方法计算loss function的效率非常高，我们只需要计算随机选择的 k 个词汇而非词汇表中的全部词汇，因此训练速度非常快。在实际中，我们使用 Noise-Contrastive Estimation（NCE） Loss，同时在TensorFlow 中也有 tf.nn.nce_loss() 直接实现了这个 loss 。\n",
    "\n",
    "<font color=red>对 NCE 的理解还是有点不明白 </font>\n",
    "\n",
    "## Skip-Gram\n",
    "\n",
    "\n",
    "**Skip-Gram的样本构造**\n",
    "\n",
    "\n",
    "以 “the quick brown fox jumpedover the lazy dog” 这句话为例。我们要构造一个语境与目标词汇的映射关系，其中语境包括一个单词左边和右边的词汇，假设我们的滑窗尺寸为1，可以制造的映射关系包括［the, brown］→quick、［quick, fox］→brown、［brown, jumped］→fox等.因为Skip-Gram模型是从目标词汇预测语境，所以训练样本不再是［the, brown］→quick，而是quick→the和quick→brown。我们的数据集就变为了（quick, the）、（quick, brown）、（brown,quick）、（brown, fox）等.\n",
    "\n",
    "我们训练时，希望模型能从目标词汇quick预测出语境the，同时也需要制造随机的词汇作为负样本（噪声），我们希望预测的概率分布在正样本the上尽可能大，而在随机产生的负样本上尽可能小。这里的做法就是通过优化算法比如 SGD 来更新模型中 Word Embedding 的参数，让概率分布的损失函数（NCE Loss）尽可能小。这样每个单词的 EmbeddedVector 就会随着训练过程不断调整，直到处于一个最适合语料的空间位置。这样我们的损失函数最小，最符合语料，同时预测出正确单词的概率也最高."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow实现词向量\n",
    "\n",
    "本节用 Skip-Gram 的方式实现 Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入库文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 收集数据\n",
    "\n",
    "### 下载数据\n",
    "\n",
    "我们先定义下载文本数据的函数。这里使用urllib.re-quest.urlretrieve下载数据的压缩文件并核对文件尺寸，如果已经下载了文件则跳过。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "# step1: Download the data\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "# 定义下载数据的函数并对数据大小进行校准\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    # local_filename = os.path.join(gettempdir(), filename)\n",
    "    local_filename = os.path.join(\"../../datasets/\", filename)\n",
    "    \n",
    "    # 如果文件不存在, 则下载文件\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename, _ = urllib.request.urlretrieve(url + filename, local_filename)\n",
    "    \n",
    "    # 获取下载文件的信息, 并进行大小验证\n",
    "    statinfo = os.stat(local_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + local_filename + '. Can you get to it with a browser?')\n",
    "    \n",
    "    return local_filename\n",
    "\n",
    "# 下载文件, 并进行大小验证 \n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解压数据\n",
    "\n",
    "接下来解压下载的压缩文件，并使用`tf.compat.as_str`将数据转成单词的列表。通过程序输出，可以知道数据最后被转为了一个包含 $17005207$个单词的列表。\n",
    "\n",
    "打印前50个单词,可以发现和文本是一一对应的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n",
      "Data [:50]  ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the']\n"
     ]
    }
   ],
   "source": [
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    \n",
    "  return data\n",
    "\n",
    "# 读取文件并把文件转化为单词列表\n",
    "vocabulary = read_data(filename)\n",
    "print('Data size', len(vocabulary))\n",
    "print('Data [:50] ', vocabulary[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立词汇表\n",
    "\n",
    "1. 使用 collections.Counter统计单词列表中单词的频数, 然后使用most_commom方法取top 50000的频数的单词作为vocabulary;\n",
    "2. 再创建一个dict, 将top 50000词汇的vocabulary放入dictionary中, 以便快速查询;\n",
    "3. 接下来讲全部单词转化为编号(以频数排序的编号), top 50000词汇意外的单词，我们可以认定其为 Unkonw(未知), 将其编号为0, 并统计这类词汇的数量；\n",
    "4. 遍历单词列表(文本单词化的列表),判断其是否出现在 dictionary 中, 如果是则转为其编号；如果不是，则转为其编号0(Unknow)；\n",
    "5. 最后返回转换后的编码(data)、每个单词的频数统计(count)、词汇表(dictionary)以及其反转形式(reverse_dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: 建立词汇表，并用UNK 代替少见的词汇\n",
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a datasets\"\"\"\n",
    "    # 统计单词top 50000的频数\n",
    "    count = [['UNK', -1]] # 统计单词出现的频数\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    \n",
    "    # 建立一个dict:{单词：编号}\n",
    "    dictionary = dict()\n",
    "    \n",
    "    # 把全部单词转化为编号\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "        \n",
    "    # 遍历单词列表(文本单词化的列表), 把单词列表整理成编号列表\n",
    "    data = list() # 文本编号化列表\n",
    "    unk_count = 0 # UNK 单词数量\n",
    "    for word in words: # 遍历文本单词话列表\n",
    "        index = dictionary.get(word, 0) # 获取单词序号\n",
    "        if index == 0: # 获取的单词为 UNK\n",
    "            unk_count += 1\n",
    "        data.append(index) # 添加单词编号\n",
    "    count[0][1] = unk_count # 统计 unk 单词数量\n",
    "    \n",
    "    # 将dictionary 反转:{编号：单词}\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    # 返回data(文本编号化序列)、count(单词频数统计列表)、词汇表({单词：编号})，反转词汇表({编号：单词})\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data, count, dictionary, reversed_dictionary = build_dataset(vocabulary, vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 然后删除原始单词列表(文本单词化列表)，以节约内存;\n",
    "2. 再打印vocabulary中最高频出现的词汇及其数量(包括UNK),可以看到UNK有418391个,最常出现的\"the\"有1061396个, 排名第二的\"of\"有593677个。打印频数最多的前10个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK)\n",
      " [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data: \n",
      " [5242, 3082, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "del vocabulary # 删除原始单词列表(文本单词化列表)\n",
    "\n",
    "print(\"Most common words (+UNK)\\n\", count[:5])\n",
    "print('Sample data: \\n', data[:10], [reversed_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成训练样本\n",
    "\n",
    "函数的使用方式如下:\n",
    "```\n",
    "batch, label = generate_batch(batch_size=8, num_skip=2, skip_windows=1)\n",
    "```\n",
    "\n",
    "假设以文本的前10个单词为例:\n",
    "\n",
    "|           |            |    |   |      |    |       |       |      |        |\n",
    "|-----------|------------|----|---|------|----|-------|-------|------|--------|\n",
    "| anarchism | originated | as | a | term | of | abuse | first | used | agaist |\n",
    "| 5244 | 3082 | 12 | 6 | 195 | 2 | 3136 | 46 | 59 | 156 |\n",
    "\n",
    "batch: $[3082, 3082, 12, 12, 6, 6, 195, 195]$\n",
    "\n",
    "label:$[[5244], [12], [3082], [6], [12], [195], [6], [2]]$\n",
    "\n",
    "转化成样本对即为:\n",
    "1. (originated, anarchism)\n",
    "2. (originated, as)\n",
    "3. (as, originated)\n",
    "4. (as, a)\n",
    "5. (a，term)\n",
    "6. (term，a)\n",
    "7. (term， of)\n",
    "8. (of，term)\n",
    "\n",
    "**名字解释**\n",
    "\n",
    "- batch_size: batch的大小;上述例子中 $batch\\_size=8$.\n",
    "- num_skips: 每个单词生成的样本数量;上述例子中$num\\_skips=2$，即每个单词生成两个样本.batch_size必须是num_skips的倍数, 确保每个batch包含一个单词所有的样本;\n",
    "- skip_window:单词最远可以联系的距离;上述例子中,skip_window=1.$num\\_skips <= 2*skip\\_windos$\n",
    "- span:滑窗大小;每个单词创建样本时, 会使用到的单词数量.上例中, $span = 2*skip\\_window + 1$.\n",
    "- data_index: 指向**文本编号化列表**中当前编号的位置，全局变量, 初始值为 $data\\_index = 0$.\n",
    "\n",
    "### 算法流程\n",
    "\n",
    "**建立变量**\n",
    "\n",
    "1. 定义单词序号 data_index 为全局变量;\n",
    "2. 建立双端队列 buffer, 容量为 span. 在对 deque 使用 append 的方法添加变量时, 只会保留最后插入的 span个变量.\n",
    "\n",
    "**给buffer建立初始值**\n",
    "\n",
    "接下来从序号data_index开始，把span个单词顺序读入buffer作为初始值。因为buffer是容量为span的deque，所以此时buffer已填充满，后续数据将替换掉前面的数据。\n",
    "\n",
    "**生成训练数据**\n",
    "\n",
    "假设我们以skip_window=1为例,即滑窗为3:\n",
    "\n",
    "1. batch是由单词的样例组成的, 单词在滑窗中间, 向左向右各自可以组成 $skip_window=1$ 个样例，总共有2个样例;\n",
    "2. skip_window 永远指向滑窗中的特征量(即样例对中左边的单词,也叫**目标单词**, 样例对中右边的单词叫**语境单词**);\n",
    "3.取出滑窗中除**目标单词**外的**语境单词**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    # 定义变量\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [skip_window target skip_window]\n",
    "    buffer = collections.deque(maxlen=span) # \n",
    "    \n",
    "    # 给buffer初始值\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span # 更新data_index\n",
    "    \n",
    "    # 生成训练数据\n",
    "    for i in range(batch_size // num_skips): # 取出每个目标单词\n",
    "        # 在滑窗中, 提取语境单词\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        \n",
    "        # 在语境单词列表中随机提取num_skips个单词\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        \n",
    "        # 提起每个语境单词,和目标单词组成样例对\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        \n",
    "        # 判断data_index是否指向最后一个单词\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "        else: \n",
    "            # 滑窗向后移动一个单词\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "            \n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    \n",
    "    # 返回batch和label\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试generate_batch()函数产生的样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3082 originated -> 5242 anarchism\n",
      "3082 originated -> 12 as\n",
      "12 as -> 6 a\n",
      "12 as -> 3082 originated\n",
      "6 a -> 195 term\n",
      "6 a -> 12 as\n",
      "195 term -> 2 of\n",
      "195 term -> 6 a\n"
     ]
    }
   ],
   "source": [
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "    print(batch[i], reversed_dictionary[batch[i]], '->', labels[i, 0], reversed_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果和上述例子的结果是一致的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "\n",
    "1. 《TensorFlow实战》\n",
    "2. github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
