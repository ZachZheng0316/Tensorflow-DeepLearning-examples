{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 理解MNIST例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入必要的库文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Load or Download MNIST > data\\mnist\n",
      "[TL] data\\mnist\\train-images-idx3-ubyte.gz\n",
      "[TL] data\\mnist\\t10k-images-idx3-ubyte.gz\n",
      "X_train.shape:(50000, 784)\n",
      "y_train.shape:(50000,)\n",
      "X_val.shape:(10000, 784)\n",
      "y_val.shape:(10000,)\n",
      "X_test.shape:(10000, 784)\n",
      "y_test.shape:(10000,)\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n",
    "\n",
    "print(\"X_train.shape:{}\".format(X_train.shape))\n",
    "print(\"y_train.shape:{}\".format(y_train.shape))\n",
    "print(\"X_val.shape:{}\".format(X_val.shape))\n",
    "print(\"y_val.shape:{}\".format(y_val.shape))\n",
    "print(\"X_test.shape:{}\".format(X_test.shape))\n",
    "print(\"y_test.shape:{}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_train.shape` 为 `(50000,784)`，可以理解成共有50000张图片并且每张图片有784个数值（像素点）。 `Y_train.shape` 为 `(50000,)` ，它是一个和 `X_train` 长度相同的向量，用于给出每幅图的数字标签，即这些图片所包含的位于0-9之间的10个数字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外对于卷积神经网络的例子，MNIST还可以按下面的4D版本来载入："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_train.shape` 为 `(50000,28,28,1)` ，这代表了50000张图片，每张图片有28行和28列。 通道为1是因为它是灰度图像，所以每个像素只能有一个值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n",
    "y_ = tf.placeholder(tf.int64, shape=[None], name='y_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] InputLayer  input: (?, 784)\n",
      "[TL] DropoutLayer drop1: keep: 0.800000 is_fix: False\n",
      "[TL] DenseLayer  relu1: 800 relu\n",
      "[TL] DropoutLayer drop2: keep: 0.500000 is_fix: False\n",
      "[TL] DenseLayer  relu2: 800 relu\n",
      "[TL] DropoutLayer drop3: keep: 0.500000 is_fix: False\n",
      "[TL] DenseLayer  output: 10 No Activation\n"
     ]
    }
   ],
   "source": [
    "# define the network\n",
    "network = tl.layers.InputLayer(x, name='input')\n",
    "network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')\n",
    "network = tl.layers.DenseLayer(network, 800, tf.nn.relu, name='relu1')\n",
    "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')\n",
    "network = tl.layers.DenseLayer(network, 800, tf.nn.relu, name='relu2')\n",
    "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')\n",
    "# the softmax is implemented internally in tl.cost.cross_entropy(y, y_) to\n",
    "# speed up computation, so we use identity here.\n",
    "# see tf.nn.sparse_softmax_cross_entropy_with_logits()\n",
    "network = tl.layers.DenseLayer(network, n_units=10, act=None, name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数和衡量指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost function and metric.\n",
    "y = network.outputs\n",
    "cost = tl.cost.cross_entropy(y, y_, name='cost')\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "y_op = tf.argmax(tf.nn.softmax(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "train_params = network.all_params\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost, var_list=train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] WARNING: From C:\\Users\\Zach\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\wrapt\\wrappers.py:523: initialize_global_variables (from tensorlayer.layers.utils) is deprecated and will be removed after 2018-09-30.\n",
      "Instructions for updating: This API is deprecated in favor of `tf.global_variables_initializer`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize all variables in the session\n",
    "tl.layers.initialize_global_variables(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL]   param   0: relu1/W:0            (784, 800)         float32_ref (mean: 0.00010284743621014059, median: 3.435601684032008e-05, std: 0.08803313970565796)   \n",
      "[TL]   param   1: relu1/b:0            (800,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   2: relu2/W:0            (800, 800)         float32_ref (mean: 0.0001710604119580239, median: 0.00013219917309470475, std: 0.08801638334989548)   \n",
      "[TL]   param   3: relu2/b:0            (800,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   4: output/W:0           (800, 10)          float32_ref (mean: -0.0009490904631093144, median: -0.00134329404681921, std: 0.08757556974887848)   \n",
      "[TL]   param   5: output/b:0           (10,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   num of params: 1276810\n",
      "[TL]   layer   0: x:0                  (?, 784)           float32\n",
      "[TL]   layer   1: drop1/mul:0          (?, 784)           float32\n",
      "[TL]   layer   2: relu1/Relu:0         (?, 800)           float32\n",
      "[TL]   layer   3: drop2/mul:0          (?, 800)           float32\n",
      "[TL]   layer   4: relu2/Relu:0         (?, 800)           float32\n",
      "[TL]   layer   5: drop3/mul:0          (?, 800)           float32\n",
      "[TL]   layer   6: output/bias_add:0    (?, 10)            float32\n"
     ]
    }
   ],
   "source": [
    "# print network information\n",
    "network.print_params()\n",
    "network.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Start training the network ...\n",
      "[TL] Epoch 1 of 500 took 0.825790s\n",
      "[TL]    val loss: 0.567263\n",
      "[TL]    val acc: 0.818300\n",
      "[TL] Epoch 5 of 500 took 0.557472s\n",
      "[TL]    val loss: 0.289096\n",
      "[TL]    val acc: 0.912000\n",
      "[TL] Epoch 10 of 500 took 0.560538s\n",
      "[TL]    val loss: 0.227771\n",
      "[TL]    val acc: 0.934200\n",
      "[TL] Epoch 15 of 500 took 0.559512s\n",
      "[TL]    val loss: 0.192987\n",
      "[TL]    val acc: 0.947100\n",
      "[TL] Epoch 20 of 500 took 0.559504s\n",
      "[TL]    val loss: 0.168367\n",
      "[TL]    val acc: 0.954500\n",
      "[TL] Epoch 25 of 500 took 0.560499s\n",
      "[TL]    val loss: 0.148946\n",
      "[TL]    val acc: 0.960200\n",
      "[TL] Epoch 30 of 500 took 0.558468s\n",
      "[TL]    val loss: 0.134236\n",
      "[TL]    val acc: 0.964400\n",
      "[TL] Epoch 35 of 500 took 0.558469s\n",
      "[TL]    val loss: 0.123387\n",
      "[TL]    val acc: 0.966000\n",
      "[TL] Epoch 40 of 500 took 0.554483s\n",
      "[TL]    val loss: 0.114395\n",
      "[TL]    val acc: 0.968800\n",
      "[TL] Epoch 45 of 500 took 0.556512s\n",
      "[TL]    val loss: 0.106742\n",
      "[TL]    val acc: 0.969900\n",
      "[TL] Epoch 50 of 500 took 0.557485s\n",
      "[TL]    val loss: 0.099622\n",
      "[TL]    val acc: 0.972000\n",
      "[TL] Epoch 55 of 500 took 0.558497s\n",
      "[TL]    val loss: 0.095608\n",
      "[TL]    val acc: 0.974100\n",
      "[TL] Epoch 60 of 500 took 0.573465s\n",
      "[TL]    val loss: 0.090782\n",
      "[TL]    val acc: 0.975100\n",
      "[TL] Epoch 65 of 500 took 0.586396s\n",
      "[TL]    val loss: 0.086170\n",
      "[TL]    val acc: 0.976200\n",
      "[TL] Epoch 70 of 500 took 0.572515s\n",
      "[TL]    val loss: 0.083011\n",
      "[TL]    val acc: 0.976400\n",
      "[TL] Epoch 75 of 500 took 0.569488s\n",
      "[TL]    val loss: 0.080604\n",
      "[TL]    val acc: 0.977200\n",
      "[TL] Epoch 80 of 500 took 0.572427s\n",
      "[TL]    val loss: 0.077402\n",
      "[TL]    val acc: 0.978000\n",
      "[TL] Epoch 85 of 500 took 0.566521s\n",
      "[TL]    val loss: 0.074572\n",
      "[TL]    val acc: 0.978300\n",
      "[TL] Epoch 90 of 500 took 0.557547s\n",
      "[TL]    val loss: 0.073380\n",
      "[TL]    val acc: 0.978900\n",
      "[TL] Epoch 95 of 500 took 0.560493s\n",
      "[TL]    val loss: 0.070634\n",
      "[TL]    val acc: 0.980100\n",
      "[TL] Epoch 100 of 500 took 0.555553s\n",
      "[TL]    val loss: 0.069759\n",
      "[TL]    val acc: 0.980500\n",
      "[TL] Epoch 105 of 500 took 0.561453s\n",
      "[TL]    val loss: 0.067669\n",
      "[TL]    val acc: 0.981000\n",
      "[TL] Epoch 110 of 500 took 0.560500s\n",
      "[TL]    val loss: 0.067172\n",
      "[TL]    val acc: 0.980900\n",
      "[TL] Epoch 115 of 500 took 0.561500s\n",
      "[TL]    val loss: 0.066232\n",
      "[TL]    val acc: 0.980400\n",
      "[TL] Epoch 120 of 500 took 0.556549s\n",
      "[TL]    val loss: 0.064383\n",
      "[TL]    val acc: 0.982000\n",
      "[TL] Epoch 125 of 500 took 0.560501s\n",
      "[TL]    val loss: 0.063717\n",
      "[TL]    val acc: 0.981700\n",
      "[TL] Epoch 130 of 500 took 0.558507s\n",
      "[TL]    val loss: 0.064245\n",
      "[TL]    val acc: 0.982400\n",
      "[TL] Epoch 135 of 500 took 0.559504s\n",
      "[TL]    val loss: 0.061922\n",
      "[TL]    val acc: 0.982400\n",
      "[TL] Epoch 140 of 500 took 0.565450s\n",
      "[TL]    val loss: 0.061756\n",
      "[TL]    val acc: 0.982500\n",
      "[TL] Epoch 145 of 500 took 0.557472s\n",
      "[TL]    val loss: 0.061664\n",
      "[TL]    val acc: 0.982000\n",
      "[TL] Epoch 150 of 500 took 0.554514s\n",
      "[TL]    val loss: 0.061032\n",
      "[TL]    val acc: 0.982600\n",
      "[TL] Epoch 155 of 500 took 0.552523s\n",
      "[TL]    val loss: 0.059836\n",
      "[TL]    val acc: 0.982500\n",
      "[TL] Epoch 160 of 500 took 0.553520s\n",
      "[TL]    val loss: 0.059059\n",
      "[TL]    val acc: 0.983000\n",
      "[TL] Epoch 165 of 500 took 0.553521s\n",
      "[TL]    val loss: 0.058526\n",
      "[TL]    val acc: 0.983400\n",
      "[TL] Epoch 170 of 500 took 0.556559s\n",
      "[TL]    val loss: 0.059414\n",
      "[TL]    val acc: 0.983700\n",
      "[TL] Epoch 175 of 500 took 0.557506s\n",
      "[TL]    val loss: 0.058208\n",
      "[TL]    val acc: 0.984400\n",
      "[TL] Epoch 180 of 500 took 0.559504s\n",
      "[TL]    val loss: 0.057390\n",
      "[TL]    val acc: 0.983800\n",
      "[TL] Epoch 185 of 500 took 0.556553s\n",
      "[TL]    val loss: 0.058003\n",
      "[TL]    val acc: 0.984100\n",
      "[TL] Epoch 190 of 500 took 0.554555s\n",
      "[TL]    val loss: 0.057198\n",
      "[TL]    val acc: 0.984100\n",
      "[TL] Epoch 195 of 500 took 0.553521s\n",
      "[TL]    val loss: 0.057261\n",
      "[TL]    val acc: 0.984100\n",
      "[TL] Epoch 200 of 500 took 0.554518s\n",
      "[TL]    val loss: 0.056918\n",
      "[TL]    val acc: 0.984600\n",
      "[TL] Epoch 205 of 500 took 0.557509s\n",
      "[TL]    val loss: 0.056638\n",
      "[TL]    val acc: 0.985000\n",
      "[TL] Epoch 210 of 500 took 0.575412s\n",
      "[TL]    val loss: 0.056258\n",
      "[TL]    val acc: 0.984600\n",
      "[TL] Epoch 215 of 500 took 0.555516s\n",
      "[TL]    val loss: 0.056264\n",
      "[TL]    val acc: 0.984400\n",
      "[TL] Epoch 220 of 500 took 0.557548s\n",
      "[TL]    val loss: 0.055950\n",
      "[TL]    val acc: 0.984000\n",
      "[TL] Epoch 225 of 500 took 0.586435s\n",
      "[TL]    val loss: 0.055325\n",
      "[TL]    val acc: 0.984400\n",
      "[TL] Epoch 230 of 500 took 0.562457s\n",
      "[TL]    val loss: 0.055307\n",
      "[TL]    val acc: 0.984800\n",
      "[TL] Epoch 235 of 500 took 0.553518s\n",
      "[TL]    val loss: 0.055329\n",
      "[TL]    val acc: 0.984900\n",
      "[TL] Epoch 240 of 500 took 0.554481s\n",
      "[TL]    val loss: 0.055014\n",
      "[TL]    val acc: 0.985300\n",
      "[TL] Epoch 245 of 500 took 0.568442s\n",
      "[TL]    val loss: 0.055159\n",
      "[TL]    val acc: 0.984800\n",
      "[TL] Epoch 250 of 500 took 0.556510s\n",
      "[TL]    val loss: 0.055276\n",
      "[TL]    val acc: 0.984800\n",
      "[TL] Epoch 255 of 500 took 0.558457s\n",
      "[TL]    val loss: 0.054533\n",
      "[TL]    val acc: 0.985400\n",
      "[TL] Epoch 260 of 500 took 0.557471s\n",
      "[TL]    val loss: 0.055025\n",
      "[TL]    val acc: 0.985600\n",
      "[TL] Epoch 265 of 500 took 0.553518s\n",
      "[TL]    val loss: 0.054752\n",
      "[TL]    val acc: 0.985500\n",
      "[TL] Epoch 270 of 500 took 0.555552s\n",
      "[TL]    val loss: 0.054524\n",
      "[TL]    val acc: 0.985400\n",
      "[TL] Epoch 275 of 500 took 0.556513s\n",
      "[TL]    val loss: 0.054990\n",
      "[TL]    val acc: 0.985400\n",
      "[TL] Epoch 280 of 500 took 0.554518s\n",
      "[TL]    val loss: 0.053829\n",
      "[TL]    val acc: 0.985400\n",
      "[TL] Epoch 285 of 500 took 0.557521s\n",
      "[TL]    val loss: 0.054772\n",
      "[TL]    val acc: 0.985500\n",
      "[TL] Epoch 290 of 500 took 0.555556s\n",
      "[TL]    val loss: 0.055598\n",
      "[TL]    val acc: 0.985100\n",
      "[TL] Epoch 295 of 500 took 0.555516s\n",
      "[TL]    val loss: 0.054335\n",
      "[TL]    val acc: 0.985500\n",
      "[TL] Epoch 300 of 500 took 0.562530s\n",
      "[TL]    val loss: 0.054364\n",
      "[TL]    val acc: 0.985900\n",
      "[TL] Epoch 305 of 500 took 0.565533s\n",
      "[TL]    val loss: 0.054406\n",
      "[TL]    val acc: 0.985600\n",
      "[TL] Epoch 310 of 500 took 0.560502s\n",
      "[TL]    val loss: 0.054165\n",
      "[TL]    val acc: 0.985500\n",
      "[TL] Epoch 315 of 500 took 0.563542s\n",
      "[TL]    val loss: 0.054918\n",
      "[TL]    val acc: 0.985600\n",
      "[TL] Epoch 320 of 500 took 0.561499s\n",
      "[TL]    val loss: 0.055291\n",
      "[TL]    val acc: 0.985400\n",
      "[TL] Epoch 325 of 500 took 0.558507s\n",
      "[TL]    val loss: 0.055651\n",
      "[TL]    val acc: 0.985000\n",
      "[TL] Epoch 330 of 500 took 0.561538s\n",
      "[TL]    val loss: 0.054689\n",
      "[TL]    val acc: 0.985600\n",
      "[TL] Epoch 335 of 500 took 0.562535s\n",
      "[TL]    val loss: 0.055729\n",
      "[TL]    val acc: 0.986100\n",
      "[TL] Epoch 340 of 500 took 0.557509s\n",
      "[TL]    val loss: 0.054953\n",
      "[TL]    val acc: 0.986200\n",
      "[TL] Epoch 345 of 500 took 0.559544s\n",
      "[TL]    val loss: 0.056525\n",
      "[TL]    val acc: 0.985500\n",
      "[TL] Epoch 350 of 500 took 0.560500s\n",
      "[TL]    val loss: 0.055108\n",
      "[TL]    val acc: 0.985700\n",
      "[TL] Epoch 355 of 500 took 0.559549s\n",
      "[TL]    val loss: 0.055196\n",
      "[TL]    val acc: 0.986100\n",
      "[TL] Epoch 360 of 500 took 0.559464s\n",
      "[TL]    val loss: 0.055804\n",
      "[TL]    val acc: 0.986100\n",
      "[TL] Epoch 365 of 500 took 0.558555s\n",
      "[TL]    val loss: 0.055854\n",
      "[TL]    val acc: 0.985400\n",
      "[TL] Epoch 370 of 500 took 0.553474s\n",
      "[TL]    val loss: 0.054853\n",
      "[TL]    val acc: 0.986000\n",
      "[TL] Epoch 375 of 500 took 0.557506s\n",
      "[TL]    val loss: 0.055233\n",
      "[TL]    val acc: 0.986500\n",
      "[TL] Epoch 380 of 500 took 0.551525s\n",
      "[TL]    val loss: 0.054979\n",
      "[TL]    val acc: 0.986400\n",
      "[TL] Epoch 385 of 500 took 0.553473s\n",
      "[TL]    val loss: 0.055321\n",
      "[TL]    val acc: 0.986000\n",
      "[TL] Epoch 390 of 500 took 0.556513s\n",
      "[TL]    val loss: 0.055499\n",
      "[TL]    val acc: 0.985800\n",
      "[TL] Epoch 395 of 500 took 0.554518s\n",
      "[TL]    val loss: 0.055746\n",
      "[TL]    val acc: 0.985500\n",
      "[TL] Epoch 400 of 500 took 0.554496s\n",
      "[TL]    val loss: 0.054830\n",
      "[TL]    val acc: 0.986100\n",
      "[TL] Epoch 405 of 500 took 0.555515s\n",
      "[TL]    val loss: 0.054676\n",
      "[TL]    val acc: 0.985900\n",
      "[TL] Epoch 410 of 500 took 0.554555s\n",
      "[TL]    val loss: 0.054875\n",
      "[TL]    val acc: 0.986500\n",
      "[TL] Epoch 415 of 500 took 0.557508s\n",
      "[TL]    val loss: 0.055184\n",
      "[TL]    val acc: 0.986500\n",
      "[TL] Epoch 420 of 500 took 0.556503s\n",
      "[TL]    val loss: 0.056068\n",
      "[TL]    val acc: 0.986000\n",
      "[TL] Epoch 425 of 500 took 0.557510s\n",
      "[TL]    val loss: 0.056293\n",
      "[TL]    val acc: 0.985900\n",
      "[TL] Epoch 430 of 500 took 0.555515s\n",
      "[TL]    val loss: 0.056220\n",
      "[TL]    val acc: 0.986500\n",
      "[TL] Epoch 435 of 500 took 0.554555s\n",
      "[TL]    val loss: 0.057118\n",
      "[TL]    val acc: 0.985500\n",
      "[TL] Epoch 440 of 500 took 0.559504s\n",
      "[TL]    val loss: 0.055386\n",
      "[TL]    val acc: 0.986500\n",
      "[TL] Epoch 445 of 500 took 0.556512s\n",
      "[TL]    val loss: 0.055551\n",
      "[TL]    val acc: 0.986100\n",
      "[TL] Epoch 450 of 500 took 0.562495s\n",
      "[TL]    val loss: 0.056927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL]    val acc: 0.986100\n",
      "[TL] Epoch 455 of 500 took 0.554517s\n",
      "[TL]    val loss: 0.056411\n",
      "[TL]    val acc: 0.986100\n",
      "[TL] Epoch 460 of 500 took 0.553558s\n",
      "[TL]    val loss: 0.056310\n",
      "[TL]    val acc: 0.986000\n",
      "[TL] Epoch 465 of 500 took 0.555547s\n",
      "[TL]    val loss: 0.055954\n",
      "[TL]    val acc: 0.986000\n",
      "[TL] Epoch 470 of 500 took 0.556512s\n",
      "[TL]    val loss: 0.056451\n",
      "[TL]    val acc: 0.986200\n",
      "[TL] Epoch 475 of 500 took 0.555510s\n",
      "[TL]    val loss: 0.055543\n",
      "[TL]    val acc: 0.986800\n",
      "[TL] Epoch 480 of 500 took 0.558540s\n",
      "[TL]    val loss: 0.056381\n",
      "[TL]    val acc: 0.986700\n",
      "[TL] Epoch 485 of 500 took 0.551525s\n",
      "[TL]    val loss: 0.055536\n",
      "[TL]    val acc: 0.986200\n",
      "[TL] Epoch 490 of 500 took 0.555514s\n",
      "[TL]    val loss: 0.055955\n",
      "[TL]    val acc: 0.986500\n",
      "[TL] Epoch 495 of 500 took 0.555560s\n",
      "[TL]    val loss: 0.056649\n",
      "[TL]    val acc: 0.986900\n",
      "[TL] Epoch 500 of 500 took 0.556512s\n",
      "[TL]    val loss: 0.056485\n",
      "[TL]    val acc: 0.986500\n",
      "[TL] Total training time: 287.444314s\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "tl.utils.fit(sess, network, train_op, cost, X_train, y_train, x, y_, acc=acc, batch_size=500, \\\n",
    "    n_epoch=500, print_freq=5, X_val=X_val, y_val=y_val, eval_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Start testing the network ...\n",
      "[TL]    test loss: 0.051042\n",
      "[TL]    test acc: 0.987400\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "tl.utils.test(sess, network, acc, X_test, y_test, x, y_, batch_size=None, cost=cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] [*] model.npz saved\n"
     ]
    }
   ],
   "source": [
    "# save the network to .npz file\n",
    "tl.files.save_npz(network.all_params, name='model.npz')\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
