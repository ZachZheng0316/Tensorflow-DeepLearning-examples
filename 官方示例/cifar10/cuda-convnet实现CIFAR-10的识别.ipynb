{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cuda-convnet实现CIFAR-10的识别\n",
    "\n",
    "## 网络结构图\n",
    "\n",
    "| Layer名称 | 描述 |\n",
    "|:----------|:------|\n",
    "|conv1 | 卷积层和ReLU激活函数 |\n",
    "|pool1 | 最大池化层 |\n",
    "|norm1 | LRN |\n",
    "|conv2 | 卷积层和ReLU激活函数 |\n",
    "|norm2 | LRN |\n",
    "|pool2 | 最大池化层 |\n",
    "|local3 | 全连接层和ReLU激活函数 |\n",
    "|local4 | 全连接层和ReLU激活函数 |\n",
    "|logist | 模型Inference的输出结果 |\n",
    "\n",
    "## 特点\n",
    "- 对weights进行L2的正则化\n",
    "- 对图片进行了翻转随机剪切等数据增强，制造更多样本\n",
    "- 在每个卷积层-最大池化层后面使用LRN层，增强模型泛化能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 载入库文件\n",
    "import cifar10, cifar10_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import re\n",
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义常用参数\n",
    "max_steps = 30 # 最大轮数\n",
    "batch_size = 30 # batch_size\n",
    "data_dir = '../../datasets/cifar10/' # 下载cifar10数据的默认路径\n",
    "DATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 初始化weight函数\n",
    "# 一般说来，L1正则会制造稀疏的特征，大部分无用特征的权重会被置0，\n",
    "# 而L2正则会让特征的权重不过大,使得特征的权重比较平均\n",
    "# 我们使用tf.nn.l2_loss函数计算weight的L2 loss\n",
    "# 再使用tf.multiply让L2 loss乘以w1，得到最后的weights loss\n",
    "# 我们接着使用tf.add_to_collection把weights统一存到一个collection钟，这个collection名为'losses'\n",
    "def variable_with_weight_loss(shape, stddev, w1):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    if w1 is not None:\n",
    "        weigth_loss = tf.multiply(tf.nn.l2_loss(var), w1, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weigth_loss)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download_and_extract():\n",
    "  \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n",
    "  dest_directory = data_dir\n",
    "  if not os.path.exists(dest_directory):\n",
    "    os.makedirs(dest_directory)\n",
    "  filename = DATA_URL.split('/')[-1]\n",
    "  filepath = os.path.join(dest_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n",
    "          float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
    "  if not os.path.exists(extracted_dir_path):\n",
    "    tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n"
     ]
    }
   ],
   "source": [
    "# 下载cifar10类数据集，并解压，展开到其默认位置\n",
    "# maybe_download_and_extract()\n",
    "\n",
    "\n",
    "# 在使用cifar10_input类中的distorted_inputs函数产生训练需要的数据集，包括特征对应\n",
    "# 的label，这里返回的是以及封装好的tensor,每次执行都会产生一个batch_size数量的样本。\n",
    "# 需要注意的是我们对数据进行了Data Augmentation(数据增强)。具体的实现细节，读者可以\n",
    "# 查看cifar10_input.distored_inputs函数，其中的数据增强操作包括随机的水平翻转\n",
    "# (tf.image.random_flip_left_right)，随机剪切一块24*24大小的图片(tf.random_crop),\n",
    "# 设置随机的亮度和对比度(tf.image.random_brightness, tf.image.random_contrast).\n",
    "# 以及对数据进行标准化(tf.image.per_image_whitening, 对数据减去均值，除以方差，保证数据零均值，方差为1)\n",
    "# 通过这些操作，我们可以获得更多的样本(带噪声的)，原来的一张图片样本可以变为多张图片\n",
    "# 相当于扩大了样本容量，对提高准确率非常有帮助。\n",
    "# 需要主要的是，我们队图像进行数据增强的操作时需要耗费大量CPU时间，因此distorted_inputs\n",
    "# 使用16个独立线程来加速任务，函数内部会产生线程池，在需要使用时会通过TensorFlow queue进行调度\n",
    "data_dir = os.path.join(data_dir, 'cifar-10-batches-bin')\n",
    "images_train, labels_train = cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=batch_size)\n",
    "\n",
    "# 我们再使用cifar10_input.inputs函数生成测试数据，这里不需要进行太多处理，不需要对图片\n",
    "# 进行翻转或者修改亮度，对比度，不过需要剪裁图片正中间的24*24大小的区块，进行数据标准化操作\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data=True,\n",
    "                                              data_dir=data_dir,\n",
    "                                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 这里创建输入数据的placeholder，包括特征和label。\n",
    "# 在设定placeholder尺寸时需要注意，因为batch_size在之后定义网络结构时被用到。\n",
    "# 所以数据尺寸中的第一个值集样本条数需要预先设定，而不是像以前一样可以设定为None\n",
    "# 而数据尺寸中的图片为24*24，即剪裁后的大小，而颜色通道则设为3,代表图片是彩色RGB三\n",
    "# 三通道\n",
    "image_holder = tf.placeholder(tf.float32, [batch_size, 24, 24, 3])\n",
    "label_holder = tf.placeholder(tf.int32, [batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 开始创建第一个卷积层\n",
    "# 第一个卷积层使用了5*5的卷积核,3个颜色通道，64个卷积核,\n",
    "# 同时设置weights初始化函数的标准差为0.05;\n",
    "# 我们不对第一个卷积层weight进行L2的正则，因此w1(weight loss)这一项设为0\n",
    "# 使用tf.nn.conv2d函数对输入数据image_holder进行卷积操作，这里的步长stride均设为1，\n",
    "# padding模式为SAME。把这一层的bias全部设为0，再将卷积的结果加上bias\n",
    "# 最后使用ReLU激活函数进行非线性化。\n",
    "# 在ReLU激活函数之后，我们使用一个尺寸为3*3且步长为2*2的最大池化层处理数据，注意这里\n",
    "# 这里最大池化层的尺寸和步长不一致，这样可以增加数据的丰富性。\n",
    "# 再之后，使用tf.nn.lrn函数，即LRN对结果进行处理。LRN最早见于Alex那篇CNN参加ImageNet\n",
    "# 比赛的论文，Alex在论文中解释LRN层模仿了生物神经系统的“侧抑制”机制，对局部神经元的活动\n",
    "# 创造竞争环境，使得其中响应比较大的值变得相对更大，并抑制其他反馈比较小的神经元，增强了\n",
    "# 模型的泛化能力\n",
    "# Alex在ImageNet数据集上的实现表明，使用LRN后CNN在top1的错误率可以降低1.4%,因此Alex在其\n",
    "# 经典的AlexNet中使用了LRN层。LRN层对ReLU种没有上限边界的激活函数会比较有用，因为它会从\n",
    "# 附近的多个卷积核的响应(Response)中挑选比较大的反馈，但是不适合Sigmiod这种有固定边界并且\n",
    "# 能抑制过大值的激活函数。\n",
    "weight1 = variable_with_weight_loss(shape=[5, 5, 3, 64], stddev=5e-2, w1=0.0)\n",
    "kernel1 = tf.nn.conv2d(image_holder, weight1, [1, 1, 1, 1], padding='SAME')\n",
    "bias1 = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "conv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "norml = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建第二个卷积层.和第一卷积层步骤很想\n",
    "# 本层卷积层的尺寸也为64\n",
    "# bias全部为0.1\n",
    "# 最后调换最大池化层和LRN层的顺序，先进行LRN层处理，再进行池化层处理\n",
    "weigth2 = variable_with_weight_loss(shape=[5, 5, 64, 64], stddev=5e-2, w1=0.0)\n",
    "kernel2 = tf.nn.conv2d(norml, weigth2, [1, 1, 1, 1], padding='SAME')\n",
    "bias2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "conv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))\n",
    "norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建全连接层\n",
    "# 隐含层个数为384\n",
    "reshape = tf.reshape(pool2, [batch_size, -1])\n",
    "dim = reshape.get_shape()[1].value\n",
    "weight3 = variable_with_weight_loss([dim, 384], stddev=0.04, w1=0.04)\n",
    "bias3 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "local3 = tf.nn.relu(tf.matmul(reshape, weight3) + bias3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建全连接层，隐含层个数192\n",
    "weight4 = variable_with_weight_loss([384, 192], stddev=0.04, w1=0.04)\n",
    "bias4 = tf.Variable(tf.constant(0.1, shape=[192]))\n",
    "local4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 最后一层硬汉层\n",
    "weight5 = variable_with_weight_loss([192, 10], stddev=1/192.0, w1=0.0)\n",
    "bias5 = tf.Variable(tf.constant(0.1,shape=[10]))\n",
    "logits = tf.add(tf.matmul(local4, weight5), bias5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义loss函数\n",
    "def loss(logits, labels):\n",
    "    # 计算结果的交叉熵\n",
    "    tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=labels, name = \"cross_entropy_per_example\")\n",
    "    # 计算结果交叉熵的均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    # 把交叉熵的均值加入'loss'集合\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    # 返回'loss'集合中的损失和\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = loss(logits=logits, labels=label_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 输出top-k的正确率\n",
    "top_k_op = tf.nn.in_top_k(logits, label_holder, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建默认的session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# 初始化所有变量\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(QueueRunnerThread-input_producer-input_producer/input_producer_EnqueueMany, started daemon 1312)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 10232)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 6984)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 1748)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 9404)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 9732)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 2392)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 7508)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 10848)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 14096)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 14692)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 7160)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 9760)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 12520)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 12072)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 11496)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 9184)>,\n",
       " <Thread(QueueRunnerThread-input/input_producer-input/input_producer/input_producer_EnqueueMany, started daemon 4284)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 11856)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 8848)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 7324)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 15032)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 2212)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 12200)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 11416)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 8780)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 14280)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 12196)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 14936)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 1372)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 3780)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 10868)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 1124)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 9592)>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 启动图片数据增强的线程队列\n",
    "tf.train.start_queue_runners()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n"
     ]
    }
   ],
   "source": [
    "# 开始进行训练\n",
    "for step in range(max_steps):\n",
    "    print(\"step: %d\" % step)\n",
    "    start_time = time.time()\n",
    "    # 获取训练数据\n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    # 进行训练\n",
    "    _, loss_value = sess.run([train_op, loss], \n",
    "                             feed_dict={image_holder: image_batch, \n",
    "                                        label_holder: label_batch})\n",
    "    # 计算训练一个batch耗费的时间\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    # 每10步打印一次数据\n",
    "    if step % 10 == 0:\n",
    "        examples_per_sec = batch_size / duration\n",
    "        sec_per_batch = float(duration)\n",
    "        \n",
    "        format_str = ('step %d, loss=%.2f (%.1f examples/sec; %.3f sec/batch)')\n",
    "        print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评测模型在测试集上的准确率\n",
    "num_examples = 10000\n",
    "import math\n",
    "num_iter = int(math.ceil(num_examples / batch_size))\n",
    "true_count = 0\n",
    "total_sample_count = num_iter * batch_size\n",
    "step = 0\n",
    "while step < 10:\n",
    "    image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "    predictions = sess.run([top_k_op], feed_dict={image_holder:image_batch,\n",
    "                                                  label_holder:label_batch})\n",
    "    true_count += np.sum(predictions)\n",
    "    step += 1\n",
    "    print(\"step: %d\" % step)\n",
    "\n",
    "# 计算最后的计算结果\n",
    "precision = true_count / total_sample_count\n",
    "print(\"precision @ 1 = %.3f\" % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
