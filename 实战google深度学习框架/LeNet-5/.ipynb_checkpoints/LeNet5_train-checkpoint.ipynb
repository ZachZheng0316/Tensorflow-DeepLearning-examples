{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import LeNet5_infernece\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1. 定义神经网络相关的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置神经网络训练参数\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.01\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "# 申明模型保存的路径\n",
    "MODEL_SAVE_PATH = \"./model/\"\n",
    "MODEL_NAME = \"LeNet-5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 定义训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    # 定义输出为4维矩阵的placeholder\n",
    "    x = tf.placeholder(tf.float32, [\n",
    "            BATCH_SIZE,                    # 第一维表示一个batch中样例的个数\n",
    "            LeNet5_infernece.IMAGE_SIZE,   # 第二维和第三维表示图片的尺寸\n",
    "            LeNet5_infernece.IMAGE_SIZE,\n",
    "            LeNet5_infernece.NUM_CHANNELS],# 第四维表示图片的深度，对于RBG格式的图片，深度为5，为什么是5\n",
    "        name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, LeNet5_infernece.OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    # 定义正则化类\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    \n",
    "    # 构建前向传播计算图\n",
    "    y = LeNet5_infernece.inference(x, True, regularizer)\n",
    "    \n",
    "    # 定义迭代变量\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE, \n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # 在会话中运行计算图\n",
    "    with tf.Session() as sess:\n",
    "        # 执行变量初始化操作\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        # 迭代优化\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # 获取MNist数据\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            \n",
    "            # 将训练的输入的训练数据格式调整为一个四维矩阵，并将这个调整后的数据传入sess.run的过程\n",
    "            reshaped_xs = np.reshape(xs, (\n",
    "                BATCH_SIZE,\n",
    "                LeNet5_infernece.IMAGE_SIZE,\n",
    "                LeNet5_infernece.IMAGE_SIZE,\n",
    "                LeNet5_infernece.NUM_CHANNELS))\n",
    "            \n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                # 输出当前训练情况。这里只输出了模型在当前batch上的损失函数大小\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "                \n",
    "                # 保存当前模型\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 主程序入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ../datasets/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ../datasets/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../datasets/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "After 1 training step(s), loss on training batch is 4.03239.\n",
      "After 1001 training step(s), loss on training batch is 0.786465.\n",
      "After 2001 training step(s), loss on training batch is 0.759091.\n",
      "After 3001 training step(s), loss on training batch is 0.680735.\n",
      "After 4001 training step(s), loss on training batch is 0.637963.\n",
      "After 5001 training step(s), loss on training batch is 0.654236.\n",
      "After 6001 training step(s), loss on training batch is 0.631651.\n",
      "After 7001 training step(s), loss on training batch is 0.675651.\n",
      "After 8001 training step(s), loss on training batch is 0.65525.\n",
      "After 9001 training step(s), loss on training batch is 0.625826.\n",
      "After 10001 training step(s), loss on training batch is 0.834089.\n",
      "After 11001 training step(s), loss on training batch is 0.613747.\n",
      "After 12001 training step(s), loss on training batch is 0.627565.\n",
      "After 13001 training step(s), loss on training batch is 0.617603.\n",
      "After 14001 training step(s), loss on training batch is 0.644178.\n",
      "After 15001 training step(s), loss on training batch is 0.637013.\n",
      "After 16001 training step(s), loss on training batch is 0.607991.\n",
      "After 17001 training step(s), loss on training batch is 0.607429.\n",
      "After 18001 training step(s), loss on training batch is 0.611535.\n",
      "After 19001 training step(s), loss on training batch is 0.618846.\n",
      "After 20001 training step(s), loss on training batch is 0.609301.\n",
      "After 21001 training step(s), loss on training batch is 0.628754.\n",
      "After 22001 training step(s), loss on training batch is 0.603723.\n",
      "After 23001 training step(s), loss on training batch is 0.610994.\n",
      "After 24001 training step(s), loss on training batch is 0.607574.\n",
      "After 25001 training step(s), loss on training batch is 0.603713.\n",
      "After 26001 training step(s), loss on training batch is 0.602034.\n",
      "After 27001 training step(s), loss on training batch is 0.599989.\n",
      "After 28001 training step(s), loss on training batch is 0.602674.\n",
      "After 29001 training step(s), loss on training batch is 0.603789.\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"../datasets/MNIST_data\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
