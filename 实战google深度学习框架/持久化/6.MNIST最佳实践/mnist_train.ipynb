{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.加载库文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import mnist_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.定义神经网络结果相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARAZTION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "# 模型保存的路径和文件名\n",
    "MODEL_SAVE_PATH = \"model/\"\n",
    "MODEL_NAME = \"model.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.定义训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    # 定义输入输出placement\n",
    "    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\"y-input\")\n",
    "    \n",
    "    # 定义正则化类\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "    \n",
    "    # 定义前向传播的过程\n",
    "    y = mnist_inference.inference(x, regularizer)\n",
    "    \n",
    "    # 定义步数变量\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 定义滑动平均类及其操作\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        MOVING_AVERAGE_DECAY, global_step)\n",
    "    variable_averages_op = variable_averages.apply(\n",
    "        tf.trainable_variables())\n",
    "    \n",
    "    # 定义交叉熵\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection(\"losses\"))\n",
    "    \n",
    "    # 定义学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY)\n",
    "    \n",
    "    # 定义优化操作\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate)\\\n",
    "        .minimize(loss, global_step=global_step)\n",
    "        \n",
    "    # 定义依赖操作\n",
    "    with tf.control_dependencies([train_step, variable_averages_op]):\n",
    "        train_op = tf.no_op(name=\"train\")\n",
    "        \n",
    "    # 初始化持久化TensorFlow持久化类\n",
    "    saver = tf.train.Saver(max_to_keep=30)\n",
    "    \n",
    "    # 定义会话\n",
    "    config = tf.ConfigProto(allow_soft_placement= True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        # 变量初始化\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        # 在训练过程中不再测试模型在验证数据上的表现，验证和测试的过程将会有一个独立\n",
    "        # 的程序来完成\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step],\n",
    "                                          feed_dict={x: xs, y_:ys})\n",
    "            \n",
    "            # 每1000轮保存一次模型\n",
    "            if i % 1000 == 0:\n",
    "                # 输出当前训练情况。\n",
    "                # 这里只输出了模型在当前训练batch上的损失函数大小\n",
    "                # 通过损失函数的大小可以大概了解训练的情况。\n",
    "                # 在验证数据集上的正确率会有一个单独的程序来生成\n",
    "                print(\"Afetr %d training step(s), loss on training \"\n",
    "                     \"batch is %g.\" % (step, loss_value))\n",
    "                \n",
    "                # 保存当前的模型\n",
    "                # 注意这里给出了global_step参数，这样可以让美俄被保存的模型的文件名\n",
    "                # 末尾加上训练的轮数，比如“model.ckpt-1000”表示训练1000轮之后得到\n",
    "                # 的模型\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME),\n",
    "                              global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.主程序入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../../../datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../../../datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../../../datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../../../datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Afetr 1 training step(s), loss on training batch is 3.47796.\n",
      "Afetr 1001 training step(s), loss on training batch is 0.188303.\n",
      "Afetr 2001 training step(s), loss on training batch is 0.210327.\n",
      "Afetr 3001 training step(s), loss on training batch is 0.149432.\n",
      "Afetr 4001 training step(s), loss on training batch is 0.131977.\n",
      "Afetr 5001 training step(s), loss on training batch is 0.118785.\n",
      "Afetr 6001 training step(s), loss on training batch is 0.119237.\n",
      "Afetr 7001 training step(s), loss on training batch is 0.0962927.\n",
      "Afetr 8001 training step(s), loss on training batch is 0.0891406.\n",
      "Afetr 9001 training step(s), loss on training batch is 0.074447.\n",
      "Afetr 10001 training step(s), loss on training batch is 0.0673602.\n",
      "Afetr 11001 training step(s), loss on training batch is 0.0642008.\n",
      "Afetr 12001 training step(s), loss on training batch is 0.0653862.\n",
      "Afetr 13001 training step(s), loss on training batch is 0.0557791.\n",
      "Afetr 14001 training step(s), loss on training batch is 0.05464.\n",
      "Afetr 15001 training step(s), loss on training batch is 0.0507933.\n",
      "Afetr 16001 training step(s), loss on training batch is 0.0474171.\n",
      "Afetr 17001 training step(s), loss on training batch is 0.0489739.\n",
      "Afetr 18001 training step(s), loss on training batch is 0.0426565.\n",
      "Afetr 19001 training step(s), loss on training batch is 0.0500439.\n",
      "Afetr 20001 training step(s), loss on training batch is 0.0391826.\n",
      "Afetr 21001 training step(s), loss on training batch is 0.0393142.\n",
      "Afetr 22001 training step(s), loss on training batch is 0.0360656.\n",
      "Afetr 23001 training step(s), loss on training batch is 0.038636.\n",
      "Afetr 24001 training step(s), loss on training batch is 0.0397253.\n",
      "Afetr 25001 training step(s), loss on training batch is 0.0405608.\n",
      "Afetr 26001 training step(s), loss on training batch is 0.0396011.\n",
      "Afetr 27001 training step(s), loss on training batch is 0.0354628.\n",
      "Afetr 28001 training step(s), loss on training batch is 0.0343353.\n",
      "Afetr 29001 training step(s), loss on training batch is 0.0333669.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    tf.reset_default_graph()\n",
    "    mnist = input_data.read_data_sets(\"../../../../datasets/MNIST_data/\", one_hot=True)\n",
    "    train(mnist)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d058f16728fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Program Files (x86)\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    122\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
