{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以全连接的方式实现ＭＮＩＳＴ识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载库文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_NODE = 784 # 输入层节点数\n",
    "LAYER1_NODE = 500 # 隐含层节点数\n",
    "OUTPUT_NODE = 10 # 输出层节点数\n",
    "BATCH_SIZE = 100 # 一个Ｂａｔｃｈ的大小：数字越小，训练过程越接近随机梯度下降；数字越大，训练过程越接近梯度下降\n",
    "LEARNING_RATE_BASE = 0.8 # 基础学习率\n",
    "LEARNING_RATE_DECAY = 0.9 #　学习率的衰减率\n",
    "REGULARIZATION_RATE = 0.0001 # 正则化系数\n",
    "TRAINING_STEPS = 30000 # 训练轮数\n",
    "MOVING_AVERAGE_DECAY = 0.99 # 滑动平均衰减率\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False) # 定义存储训练轮数的变量，属性为不可训练类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定神经网络的输入：输入层数据, 是否使用滑动平均模型的标志, 输入层的权重, 输入层的偏置量, 隐含层的权重, 隐含层的偏置量。在这里定义了一个使用 $ReLU$ 激活函数的三层全连接神经网络。返回:前向传播计算的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(input_tensor, avg_class,  weights1, biases1, weights2, biases2):\n",
    "    # 当没有提供滑动平均类时，直接使用参数当前的取值\n",
    "    if avg_class == None:\n",
    "        # 计算隐含层前向传播结果\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        \n",
    "        # 计算输出层的前向传播结果.\n",
    "        # 因为在计算损失函数时，会一并计算softmax函数，此处不需要加入激活函数\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    \n",
    "    else:\n",
    "        # 首先, 使用avg_class.average函数来计算出变量的滑动平均值\n",
    "        # 然后，再计算相应的神经网络前向传播结果\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        \n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(mnist):\n",
    "    # 建立输入数据和标签的占位符\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name=\"y-input\")\n",
    "    \n",
    "    # 生成隐含层和输出层参数\n",
    "    weight1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    bias1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "    weight2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    bias2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    # 计算在当前条件下神经网路的前向传播结果\n",
    "    # 此处不使用滑动平均类\n",
    "    y = inference(input_tensor=x, avg_class=None, weights1=weight1, biases1=bias1, weights2=weight2, biases2=bias2)\n",
    "    \n",
    "    \n",
    "    # 定义滑动平均类和滑动平均操作\n",
    "    # 对所有代表神经网络的参数的变量上使用滑动平均类。tf.trainable_variables返回的就是图上集合ＧraphKeys.TRAINABLE_VARABLEＳ中的元素\n",
    "    # 这个集合的元素就是所有没有指定trainable=False\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    # 重新计算前向传播结果\n",
    "    # 此处使用滑动平均类：滑动平均操作不会改变变量本身的取值，而是会维护一个影子变量来记录其滑动平均值\n",
    "    average_y = inference(input_tensor=x, avg_class=variable_averages, weights1=weight1, biases1=bias1, weights2=weight2, biases2=bias2)\n",
    "    \n",
    "    # 计算损失函数：交叉熵作为刻画预测值与损失值之间差距的损失函数\n",
    "    # 这里使用TensorFlow中提供的sparse_softmax_cross_entroy_with_logitis函数来计算交叉熵。\n",
    "    # 当分类问题只有一个正确答案时，可以使用这个函数来加速交叉熵的计算。这个函数的第一个参数是神经网络\n",
    "    # 不包括softmax层的前向传播结果，第二个参数是训练数据的第二个答案。\n",
    "    # 因为标准答案是一个长度为１０的一维数组,而该答案需要提供一个正确答案的数字，所以需要使用tf.argmax\n",
    "    # 函数来得到正确答案对应的类别编号。\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_, 1), logits=y)\n",
    "    \n",
    "    # 计算在当前batch下，所有样例交叉熵的平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 建立Ｌ２正则化类，并计算正则化值\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularization = regularizer(weight1) + regularizer(weight2)\n",
    "    \n",
    "    # 计算总损失函数\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    # 设置指数衰减的学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE, # 基础学习率，随着迭代的进行，更新变量时使用的学习率在这个基础上递减\n",
    "        global_step, # 当前迭代的轮数\n",
    "        mnist.train.num_examples / BATCH_SIZE, # 过完所有的训练数据需要迭代的次数\n",
    "        LEARNING_RATE_DECAY) # 学习速率的衰减速度\n",
    "    \n",
    "    # 优化损失函数\n",
    "    # 这里的损失函数包含了交叉熵和正则化项\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # 在训练神经网络模型时，每过一遍数据即需要通过反向传播来更新神经网络中的参数。\n",
    "    # 又需要更新每一个参数的滑动平均值\n",
    "    # 为了一次完成多个操作，ＴensorFlow提供了tf.control_dependencies和tf.group两种机制\n",
    "    # 下面两行程序和train_op = tf.group(train_step, variables_averages_op)等价\n",
    "    with tf.control_dependencies([train_step, variable_averages_op]):\n",
    "        train_op = tf.no_op(name = \"train\")\n",
    "        \n",
    "    # 检验使用滑动平均模型的神经网络前向传播结果是否正确\n",
    "    # tf.argmax(average_y, 1):计算每个样例的预测答案\n",
    "    # 其中average_y是一个batch_size*10的二维数组，每一行表示一个样例的前向传播结果。\n",
    "    # tf.argmax的第二个参数\"1\"表示选取最大值的操作仅在第一个维度中进行，也就是说，只在没一行选取最大值对应的下标\n",
    "    # 于是得到的结果时一个长度为batch的一维数组，其中的值表示识别的结果\n",
    "    # tf.equal判断两个张量的每一维度是否相等，相等的元素返回Tｒue,不相等的元素返回False\n",
    "    # 将一维bool型的数值转化为实数，然后计算平均值\n",
    "    # 这个平均值就是模型在这一组数据上的精度\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # 初始化回话并训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        # 准备验证数据\n",
    "        # 一般在神经网络的训练过程中会通过验证数据大致判断停止的条件和评判训练的结果\n",
    "        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "        \n",
    "        # 准备测试数据\n",
    "        # 在真实的应用中，这部分数据是不可见的，这个数据只是作为模型优劣最后的评判标准\n",
    "        test_feed = {x: mnist.test.images, y_:mnist.test.labels}\n",
    "        \n",
    "        # 迭代地训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # 没１０００次输出一次在验证集上的结果\n",
    "            if i % 1000 == 0:\n",
    "                # 计算滑动平均模型在验证集上的结果\n",
    "                # 因为MNIST的验证集比较小，所以一次可以处理所有的验证集。\n",
    "                # 为了计算方便，本样例程序没有将验证集划分为更小的batch；\n",
    "                # 当神经网络模型比较复杂或者验证数据集比较大时，太大的batch可能会导致计算时间过长或者内存溢出\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step(s), validation accuracy \" \"using average model is %g\" % (i, validate_acc))\n",
    "                \n",
    "            # 产生这一轮使用batch的数据集, 并训练过程\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_:ys})\n",
    "            \n",
    "        # 在结束训练之后，在测试数据上最终检测模型的正确率\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print (\"After %d training step(s), test accuracy \" \"model is %g\" % (TRAINING_STEPS, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主程序入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "After 0 training step(s), validation accuracy using average model is 0.1352\n",
      "After 1000 training step(s), validation accuracy using average model is 0.9758\n",
      "After 2000 training step(s), validation accuracy using average model is 0.9804\n",
      "After 3000 training step(s), validation accuracy using average model is 0.9804\n",
      "After 4000 training step(s), validation accuracy using average model is 0.9814\n",
      "After 5000 training step(s), validation accuracy using average model is 0.9806\n",
      "After 6000 training step(s), validation accuracy using average model is 0.9804\n",
      "After 7000 training step(s), validation accuracy using average model is 0.9824\n",
      "After 8000 training step(s), validation accuracy using average model is 0.9814\n",
      "After 9000 training step(s), validation accuracy using average model is 0.9816\n",
      "After 10000 training step(s), validation accuracy using average model is 0.9826\n",
      "After 11000 training step(s), validation accuracy using average model is 0.9822\n",
      "After 12000 training step(s), validation accuracy using average model is 0.9818\n",
      "After 13000 training step(s), validation accuracy using average model is 0.982\n",
      "After 14000 training step(s), validation accuracy using average model is 0.9824\n",
      "After 15000 training step(s), validation accuracy using average model is 0.982\n",
      "After 16000 training step(s), validation accuracy using average model is 0.9824\n",
      "After 17000 training step(s), validation accuracy using average model is 0.9824\n",
      "After 18000 training step(s), validation accuracy using average model is 0.982\n",
      "After 19000 training step(s), validation accuracy using average model is 0.982\n",
      "After 20000 training step(s), validation accuracy using average model is 0.982\n",
      "After 21000 training step(s), validation accuracy using average model is 0.9822\n",
      "After 22000 training step(s), validation accuracy using average model is 0.9822\n",
      "After 23000 training step(s), validation accuracy using average model is 0.982\n",
      "After 24000 training step(s), validation accuracy using average model is 0.9822\n",
      "After 25000 training step(s), validation accuracy using average model is 0.9822\n",
      "After 26000 training step(s), validation accuracy using average model is 0.9822\n",
      "After 27000 training step(s), validation accuracy using average model is 0.9822\n",
      "After 28000 training step(s), validation accuracy using average model is 0.9822\n",
      "After 29000 training step(s), validation accuracy using average model is 0.9822\n",
      "After 30000 training step(s), test accuracy model is 0.9817\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zach/anaconda2/envs/tensorflowpy27/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "    # 下载或读取数据\n",
    "    mnist = input_data.read_data_sets(\"../datasets/MNIST_data/\", one_hot=True)\n",
    "    train(mnist)\n",
    "    \n",
    "# TensorFloｗ提供的一个主程序入口，tf.app.run会调用上面定义的main函数\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
